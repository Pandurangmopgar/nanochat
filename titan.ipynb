{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üß† Titans Neural Memory ‚Äî GPU Training (Colab)\n",
                "\n",
                "This notebook trains a GPT model with Titans MAL (Memory as Layer) neural memory\n",
                "on a real text dataset using a T4 GPU (15GB VRAM).\n",
                "\n",
                "**What this validates:**\n",
                "- Loss decreases steadily on real text (not random)\n",
                "- Memory VRAM usage stays stable (no leaks)\n",
                "- Mixed precision (bfloat16) is numerically stable\n",
                "- Inner-loop memory updates work correctly on GPU\n",
                "\n",
                "**Runtime:** ~10-15 minutes for 500 steps"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup & Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check GPU\n",
                "!nvidia-smi\n",
                "import torch\n",
                "print(f\"\\nPyTorch: {torch.__version__}\")\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_mem / 1024**3:.1f} GB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "!pip install -q datasets tiktoken wandb"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Clone Repository"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "if not os.path.exists('nanochat'):\n",
                "    !git clone https://github.com/Pandurangmopgar/nanochat.git\n",
                "os.chdir('nanochat')\n",
                "!git log --oneline -5"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Import Model Components\n",
                "\n",
                "We import directly from the source ‚Äî no Rust/maturin build needed for training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "sys.path.insert(0, '.')\n",
                "\n",
                "import time\n",
                "import math\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from dataclasses import dataclass\n",
                "\n",
                "# Import our Titans modules directly\n",
                "from nanochat.titans_memory import TitansMemory, TitansLayer\n",
                "from nanochat.gpt import GPT, GPTConfig\n",
                "\n",
                "print(\"‚úÖ Imports successful\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Quick Memory Validation on GPU"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "print(f\"Using device: {device}\")\n",
                "\n",
                "# Quick sanity check: memory learns on GPU\n",
                "mem = TitansMemory(model_dim=128, memory_dim=64, memory_depth=2).to(device).train()\n",
                "# Boost learning rate for visible memorization\n",
                "nn.init.constant_(mem.proj_theta.bias, 0.0)  # softplus(0) ‚âà 0.69\n",
                "\n",
                "pattern = torch.randn(1, 8, 128, device=device)\n",
                "\n",
                "# Measure surprise before\n",
                "with torch.no_grad():\n",
                "    x_conv = mem.conv(pattern.transpose(1, 2)).transpose(1, 2)\n",
                "    k, v = mem.W_K(x_conv), mem.W_V(x_conv)\n",
                "    loss_before = F.mse_loss(mem.memory_net(k).float(), v.float()).item()\n",
                "\n",
                "# Feed pattern 50 times\n",
                "for _ in range(50):\n",
                "    mem(pattern, update_memory=True)\n",
                "\n",
                "# Measure surprise after\n",
                "with torch.no_grad():\n",
                "    x_conv = mem.conv(pattern.transpose(1, 2)).transpose(1, 2)\n",
                "    k, v = mem.W_K(x_conv), mem.W_V(x_conv)\n",
                "    loss_after = F.mse_loss(mem.memory_net(k).float(), v.float()).item()\n",
                "\n",
                "print(f\"Surprise before: {loss_before:.6f}\")\n",
                "print(f\"Surprise after:  {loss_after:.6f}\")\n",
                "print(f\"Reduction: {(1 - loss_after/loss_before)*100:.1f}%\")\n",
                "print(f\"‚úÖ Memory is learning on {device.upper()}!\")\n",
                "\n",
                "del mem, pattern  # free VRAM\n",
                "torch.cuda.empty_cache()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Load Real Text Data\n",
                "\n",
                "We use WikiText-103 ‚Äî a standard language modeling benchmark.\n",
                "Using tiktoken (GPT-2 tokenizer) since the Rust BPE tokenizer requires maturin build."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from datasets import load_dataset\n",
                "import tiktoken\n",
                "\n",
                "# Load WikiText-103\n",
                "print(\"Loading WikiText-103...\")\n",
                "dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"train\")\n",
                "val_dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"validation\")\n",
                "\n",
                "# Tokenize with tiktoken (GPT-2 compatible)\n",
                "enc = tiktoken.get_encoding(\"gpt2\")\n",
                "VOCAB_SIZE = enc.n_vocab  # 50257, padded to 50304 for efficiency\n",
                "VOCAB_SIZE_PADDED = 50304\n",
                "\n",
                "print(f\"Tokenizer vocab size: {VOCAB_SIZE} (padded to {VOCAB_SIZE_PADDED})\")\n",
                "print(f\"Train documents: {len(dataset):,}\")\n",
                "print(f\"Val documents: {len(val_dataset):,}\")\n",
                "\n",
                "# Pre-tokenize everything into a flat tensor for fast batching\n",
                "print(\"Tokenizing train split...\")\n",
                "all_train_tokens = []\n",
                "for i, doc in enumerate(dataset):\n",
                "    text = doc['text'].strip()\n",
                "    if text:\n",
                "        all_train_tokens.extend(enc.encode(text))\n",
                "    if i % 100000 == 0 and i > 0:\n",
                "        print(f\"  Processed {i:,} docs, {len(all_train_tokens):,} tokens so far...\")\n",
                "\n",
                "print(f\"\\nTokenizing val split...\")\n",
                "all_val_tokens = []\n",
                "for doc in val_dataset:\n",
                "    text = doc['text'].strip()\n",
                "    if text:\n",
                "        all_val_tokens.extend(enc.encode(text))\n",
                "\n",
                "train_tokens = torch.tensor(all_train_tokens, dtype=torch.long)\n",
                "val_tokens = torch.tensor(all_val_tokens, dtype=torch.long)\n",
                "\n",
                "print(f\"\\n‚úÖ Train tokens: {len(train_tokens):,}\")\n",
                "print(f\"‚úÖ Val tokens: {len(val_tokens):,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Data loader: yields random batches from the token buffer\n",
                "def get_batch(split, batch_size, seq_len, device):\n",
                "    data = train_tokens if split == 'train' else val_tokens\n",
                "    ix = torch.randint(len(data) - seq_len - 1, (batch_size,))\n",
                "    x = torch.stack([data[i:i+seq_len] for i in ix]).to(device, dtype=torch.int32)\n",
                "    y = torch.stack([data[i+1:i+seq_len+1] for i in ix]).to(device, dtype=torch.long)\n",
                "    return x, y\n",
                "\n",
                "# Quick test\n",
                "x, y = get_batch('train', 2, 64, device)\n",
                "print(f\"Batch shape: x={x.shape}, y={y.shape}\")\n",
                "print(f\"Sample decoded: {enc.decode(x[0, :20].tolist())}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Initialize Model\n",
                "\n",
                "**Small config for T4 (15GB VRAM):**\n",
                "- 4 layers, 384 dim, 6 heads ‚Üí ~15M params (fits easily)\n",
                "- seq_len=512 to keep VRAM manageable\n",
                "- Titans memory: dim=256, depth=2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Model configuration (fits in T4 15GB)\n",
                "SEQ_LEN = 512\n",
                "BATCH_SIZE = 8  # Adjust down if OOM\n",
                "GRAD_ACCUM = 4  # Effective batch = 8 * 4 = 32\n",
                "\n",
                "config = GPTConfig(\n",
                "    sequence_len=SEQ_LEN,\n",
                "    vocab_size=VOCAB_SIZE_PADDED,\n",
                "    n_layer=4,\n",
                "    n_head=6,\n",
                "    n_kv_head=6,\n",
                "    n_embd=384,\n",
                "    # Titans memory (MAL variant)\n",
                "    use_titans=True,\n",
                "    titans_memory_dim=256,\n",
                "    titans_memory_depth=2,\n",
                ")\n",
                "\n",
                "# Initialize model on GPU\n",
                "model = GPT(config).to(device)\n",
                "model.init_weights()\n",
                "\n",
                "# Parameter counts\n",
                "total_params = sum(p.numel() for p in model.parameters())\n",
                "titans_params = sum(p.numel() for p in model.titans_layer.parameters()) if model.titans_layer else 0\n",
                "other_params = total_params - titans_params\n",
                "\n",
                "print(f\"Total parameters: {total_params:,}\")\n",
                "print(f\"  Transformer:    {other_params:,}\")\n",
                "print(f\"  Titans memory:  {titans_params:,}\")\n",
                "print(f\"  Memory fraction: {titans_params/total_params*100:.1f}%\")\n",
                "print(f\"\\nVRAM used after init: {torch.cuda.memory_allocated()/1024**2:.0f} MB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Training Loop\n",
                "\n",
                "500 steps with AdamW, mixed precision, gradient clipping.\n",
                "Evaluates validation loss every 50 steps."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training hyperparameters\n",
                "NUM_STEPS = 500\n",
                "LR = 3e-4\n",
                "WARMUP_STEPS = 50\n",
                "GRAD_CLIP = 1.0\n",
                "EVAL_EVERY = 50\n",
                "EVAL_STEPS = 10  # batches for val loss estimate\n",
                "\n",
                "# Optimizer\n",
                "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, betas=(0.9, 0.95), weight_decay=0.1)\n",
                "\n",
                "# LR scheduler: linear warmup then cosine decay\n",
                "def get_lr(step):\n",
                "    if step < WARMUP_STEPS:\n",
                "        return LR * (step + 1) / WARMUP_STEPS\n",
                "    progress = (step - WARMUP_STEPS) / max(1, NUM_STEPS - WARMUP_STEPS)\n",
                "    return LR * 0.5 * (1 + math.cos(math.pi * progress))\n",
                "\n",
                "# Mixed precision (use float16 for T4, bfloat16 for A100+)\n",
                "use_amp = device == 'cuda'\n",
                "amp_dtype = torch.float16 if torch.cuda.get_device_capability()[0] < 8 else torch.bfloat16\n",
                "scaler = torch.amp.GradScaler(enabled=(amp_dtype == torch.float16))\n",
                "print(f\"AMP dtype: {amp_dtype}\")\n",
                "print(f\"Grad scaler: {'enabled' if amp_dtype == torch.float16 else 'disabled'}\")\n",
                "\n",
                "# Evaluation function\n",
                "@torch.no_grad()\n",
                "def estimate_val_loss():\n",
                "    model.eval()\n",
                "    losses = []\n",
                "    for _ in range(EVAL_STEPS):\n",
                "        x, y = get_batch('val', BATCH_SIZE, SEQ_LEN, device)\n",
                "        with torch.amp.autocast(device_type='cuda', dtype=amp_dtype, enabled=use_amp):\n",
                "            loss = model(x, y)\n",
                "        losses.append(loss.item())\n",
                "    model.train()\n",
                "    return sum(losses) / len(losses)\n",
                "\n",
                "print(f\"\\nConfig: {NUM_STEPS} steps, batch={BATCH_SIZE}x{GRAD_ACCUM}={BATCH_SIZE*GRAD_ACCUM}, seq_len={SEQ_LEN}\")\n",
                "print(f\"Tokens/step: {BATCH_SIZE * GRAD_ACCUM * SEQ_LEN:,}\")\n",
                "print(f\"Total tokens: {BATCH_SIZE * GRAD_ACCUM * SEQ_LEN * NUM_STEPS:,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ==================== TRAINING ====================\n",
                "model.train()\n",
                "train_losses = []\n",
                "val_losses = []\n",
                "step_times = []\n",
                "\n",
                "print(\"Starting training...\")\n",
                "print(f\"{'Step':>6} | {'Train Loss':>10} | {'Val Loss':>10} | {'LR':>10} | {'ms/step':>8} | {'VRAM MB':>8}\")\n",
                "print(\"-\" * 75)\n",
                "\n",
                "# Initial val loss\n",
                "val_loss = estimate_val_loss()\n",
                "val_losses.append((0, val_loss))\n",
                "print(f\"{'0':>6} | {'---':>10} | {val_loss:>10.4f} | {'---':>10} | {'---':>8} | {torch.cuda.memory_allocated()/1024**2:>8.0f}\")\n",
                "\n",
                "for step in range(1, NUM_STEPS + 1):\n",
                "    t0 = time.time()\n",
                "\n",
                "    # Set learning rate\n",
                "    lr = get_lr(step)\n",
                "    for param_group in optimizer.param_groups:\n",
                "        param_group['lr'] = lr\n",
                "\n",
                "    # Gradient accumulation loop\n",
                "    optimizer.zero_grad(set_to_none=True)\n",
                "    total_loss = 0.0\n",
                "\n",
                "    for micro_step in range(GRAD_ACCUM):\n",
                "        x, y = get_batch('train', BATCH_SIZE, SEQ_LEN, device)\n",
                "        with torch.amp.autocast(device_type='cuda', dtype=amp_dtype, enabled=use_amp):\n",
                "            loss = model(x, y)\n",
                "        loss_scaled = loss / GRAD_ACCUM\n",
                "        scaler.scale(loss_scaled).backward()\n",
                "        total_loss += loss.item()\n",
                "\n",
                "    # Gradient clipping\n",
                "    scaler.unscale_(optimizer)\n",
                "    torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
                "\n",
                "    # Optimizer step\n",
                "    scaler.step(optimizer)\n",
                "    scaler.update()\n",
                "\n",
                "    torch.cuda.synchronize()\n",
                "    t1 = time.time()\n",
                "    dt_ms = (t1 - t0) * 1000\n",
                "    step_times.append(dt_ms)\n",
                "\n",
                "    avg_loss = total_loss / GRAD_ACCUM\n",
                "    train_losses.append((step, avg_loss))\n",
                "\n",
                "    # Evaluate periodically\n",
                "    if step % EVAL_EVERY == 0 or step == NUM_STEPS:\n",
                "        val_loss = estimate_val_loss()\n",
                "        val_losses.append((step, val_loss))\n",
                "        vram_mb = torch.cuda.memory_allocated() / 1024**2\n",
                "        peak_mb = torch.cuda.max_memory_allocated() / 1024**2\n",
                "        print(f\"{step:>6} | {avg_loss:>10.4f} | {val_loss:>10.4f} | {lr:>10.6f} | {dt_ms:>8.1f} | {vram_mb:>8.0f}\")\n",
                "\n",
                "    # Progress printout every 10 steps\n",
                "    elif step % 10 == 0:\n",
                "        print(f\"{step:>6} | {avg_loss:>10.4f} | {'---':>10} | {lr:>10.6f} | {dt_ms:>8.1f} | {'':>8}\", end='\\r')\n",
                "\n",
                "# ==================== DONE ====================\n",
                "print(f\"\\n{'='*75}\")\n",
                "print(f\"Training complete!\")\n",
                "print(f\"Peak VRAM: {torch.cuda.max_memory_allocated()/1024**2:.0f} MB\")\n",
                "print(f\"Avg step time: {sum(step_times)/len(step_times):.1f} ms\")\n",
                "print(f\"Best val loss: {min(v for _, v in val_losses):.4f}\")\n",
                "print(f\"Final val loss: {val_losses[-1][1]:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Training Curves"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Training loss (smoothed)\n",
                "steps_t = [s for s, _ in train_losses]\n",
                "losses_t = [l for _, l in train_losses]\n",
                "# EMA smoothing\n",
                "smooth = [losses_t[0]]\n",
                "for l in losses_t[1:]:\n",
                "    smooth.append(0.95 * smooth[-1] + 0.05 * l)\n",
                "\n",
                "ax1.plot(steps_t, losses_t, alpha=0.2, color='blue', label='Raw')\n",
                "ax1.plot(steps_t, smooth, color='blue', linewidth=2, label='Smoothed')\n",
                "ax1.set_xlabel('Step')\n",
                "ax1.set_ylabel('Train Loss')\n",
                "ax1.set_title('Training Loss')\n",
                "ax1.legend()\n",
                "ax1.grid(True, alpha=0.3)\n",
                "\n",
                "# Validation loss\n",
                "steps_v = [s for s, _ in val_losses]\n",
                "losses_v = [l for _, l in val_losses]\n",
                "ax2.plot(steps_v, losses_v, 'ro-', linewidth=2, markersize=6)\n",
                "ax2.set_xlabel('Step')\n",
                "ax2.set_ylabel('Val Loss')\n",
                "ax2.set_title('Validation Loss')\n",
                "ax2.grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('titans_training_curves.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "print(\"\\nüìä Saved: titans_training_curves.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Memory Module Inspection"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Inspect the trained memory module\n",
                "mem = model.titans_layer.memory\n",
                "\n",
                "print(\"=== Titans Memory State ===\")\n",
                "print(f\"\\nüìä Data-dependent hyperparameters (on a random batch):\")\n",
                "x_test = torch.randn(1, 64, 384, device=device)\n",
                "alpha, eta, theta = mem._compute_hyperparams(x_test)\n",
                "print(f\"  alpha (forgetting rate):  {alpha.item():.6f}\")\n",
                "print(f\"  eta (momentum decay):     {eta.item():.6f}\")\n",
                "print(f\"  theta (learning rate):    {theta.item():.6f}\")\n",
                "\n",
                "print(f\"\\nüîë K/V Projection norms:\")\n",
                "print(f\"  W_K norm: {mem.W_K.weight.norm().item():.4f}\")\n",
                "print(f\"  W_V norm: {mem.W_V.weight.norm().item():.4f}\")\n",
                "\n",
                "print(f\"\\nüß† Memory MLP weight norms per layer:\")\n",
                "for i, (name, p) in enumerate(mem.memory_net.named_parameters()):\n",
                "    print(f\"  {name}: norm={p.norm().item():.4f}, mean={p.mean().item():.6f}, std={p.std().item():.6f}\")\n",
                "\n",
                "print(f\"\\nüö™ Gate statistics:\")\n",
                "gate = model.titans_layer.gate\n",
                "print(f\"  Weight norm: {gate.weight.norm().item():.4f}\")\n",
                "print(f\"  Bias mean: {gate.bias.mean().item():.4f} (sigmoid = {torch.sigmoid(gate.bias).mean().item():.4f})\")\n",
                "\n",
                "print(f\"\\nüíæ Momentum state:\")\n",
                "if mem._momentum_state is not None:\n",
                "    ms = mem._momentum_state\n",
                "    print(f\"  Shape: {ms.shape}\")\n",
                "    print(f\"  Norm: {ms.norm().item():.6f}\")\n",
                "    print(f\"  Non-zero: {(ms.abs() > 1e-8).sum().item()} / {ms.numel()}\")\n",
                "else:\n",
                "    print(f\"  Not initialized (eval mode or no forward pass)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Generate Text"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simple greedy generation\n",
                "model.eval()\n",
                "\n",
                "prompts = [\n",
                "    \"The capital of France is\",\n",
                "    \"In the year 2025,\",\n",
                "    \"The largest planet in our solar system\",\n",
                "    \"Once upon a time\",\n",
                "]\n",
                "\n",
                "for prompt_text in prompts:\n",
                "    tokens = enc.encode(prompt_text)\n",
                "    x = torch.tensor([tokens], dtype=torch.long, device=device)\n",
                "\n",
                "    with torch.no_grad(), torch.amp.autocast(device_type='cuda', dtype=amp_dtype, enabled=use_amp):\n",
                "        for _ in range(50):\n",
                "            # Only use last seq_len tokens\n",
                "            x_cond = x[:, -SEQ_LEN:].to(torch.int32)\n",
                "            logits = model(x_cond)\n",
                "            next_token = logits[:, -1, :].argmax(dim=-1, keepdim=True).long()\n",
                "            x = torch.cat([x, next_token], dim=1)\n",
                "\n",
                "    generated = enc.decode(x[0].tolist())\n",
                "    print(f\"\\nüí¨ {prompt_text}\")\n",
                "    print(f\"   {generated}\")\n",
                "\n",
                "model.train()\n",
                "print(\"\\n‚ö†Ô∏è Note: Generation quality requires much longer training (100K+ steps).\")\n",
                "print(\"The goal here is to verify the model runs without errors, not to produce good text.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. VRAM Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=== VRAM Summary ===\")\n",
                "print(f\"Current allocated: {torch.cuda.memory_allocated()/1024**2:.0f} MB\")\n",
                "print(f\"Peak allocated:    {torch.cuda.max_memory_allocated()/1024**2:.0f} MB\")\n",
                "print(f\"Current reserved:  {torch.cuda.memory_reserved()/1024**2:.0f} MB\")\n",
                "print(f\"Peak reserved:     {torch.cuda.max_memory_reserved()/1024**2:.0f} MB\")\n",
                "\n",
                "total_vram = torch.cuda.get_device_properties(0).total_mem / 1024**2\n",
                "peak = torch.cuda.max_memory_allocated() / 1024**2\n",
                "print(f\"\\nUsage: {peak:.0f} / {total_vram:.0f} MB ({peak/total_vram*100:.1f}% of VRAM)\")\n",
                "\n",
                "if peak / total_vram < 0.8:\n",
                "    print(\"\\nüí° Tip: You have headroom. Try increasing BATCH_SIZE or n_layer for better quality.\")\n",
                "elif peak / total_vram > 0.95:\n",
                "    print(\"\\n‚ö†Ô∏è Close to VRAM limit. Reduce BATCH_SIZE if you see OOM errors.\")\n",
                "else:\n",
                "    print(\"\\n‚úÖ VRAM usage is healthy.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 12. Save Checkpoint (Optional)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save the trained model\n",
                "checkpoint = {\n",
                "    'model_state_dict': model.state_dict(),\n",
                "    'optimizer_state_dict': optimizer.state_dict(),\n",
                "    'config': {\n",
                "        'sequence_len': SEQ_LEN,\n",
                "        'vocab_size': VOCAB_SIZE_PADDED,\n",
                "        'n_layer': 4,\n",
                "        'n_head': 6,\n",
                "        'n_kv_head': 6,\n",
                "        'n_embd': 384,\n",
                "        'use_titans': True,\n",
                "        'titans_memory_dim': 256,\n",
                "        'titans_memory_depth': 2,\n",
                "    },\n",
                "    'train_losses': train_losses,\n",
                "    'val_losses': val_losses,\n",
                "    'step': NUM_STEPS,\n",
                "}\n",
                "torch.save(checkpoint, 'titans_checkpoint.pt')\n",
                "print(f\"‚úÖ Checkpoint saved: titans_checkpoint.pt ({os.path.getsize('titans_checkpoint.pt')/1024/1024:.1f} MB)\")\n",
                "\n",
                "# Download from Colab\n",
                "try:\n",
                "    from google.colab import files\n",
                "    files.download('titans_checkpoint.pt')\n",
                "    files.download('titans_training_curves.png')\n",
                "except ImportError:\n",
                "    print(\"Not in Colab ‚Äî files saved locally.\")"
            ]
        }
    ]
}